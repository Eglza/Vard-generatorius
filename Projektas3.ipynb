{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLApBcj6IlhqscfBm6Q7yb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eglza/Vard-generatorius/blob/main/Projektas3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lietuviškų vardų generatorius**\n"
      ],
      "metadata": {
        "id": "nxoCcqyr63la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOGq0-5OFSHF",
        "outputId": "1d716b08-b3c1-41c9-e794-3284b4efbbd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.2 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Create function to scrape and save names\n",
        "def download_names():\n",
        "    names_male = []\n",
        "    names_female = []\n",
        "\n",
        "    for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n",
        "        url = f'https://vardai.vlkk.lt/sarasas/{key}/'\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Scrape male names\n",
        "        links_male = soup.find_all('a', class_='names_list__links names_list__links--man')\n",
        "        names_male += [name.text for name in links_male]\n",
        "\n",
        "        # Scrape female names\n",
        "        links_female = soup.find_all('a', class_='names_list__links names_list__links--woman')\n",
        "        names_female += [name.text for name in links_female]\n",
        "\n",
        "    # Save male names to a text file\n",
        "    with open('vardai_male.txt', 'w', encoding='utf-8') as f:\n",
        "        for name in names_male:\n",
        "            f.write(f\"{name}\\n\")\n",
        "\n",
        "    # Save female names to a text file\n",
        "    with open('vardai_female.txt', 'w', encoding='utf-8') as f:\n",
        "        for name in names_female:\n",
        "            f.write(f\"{name}\\n\")\n",
        "\n",
        "    print(f\"Downloaded {len(names_male)} male names and {len(names_female)} female names.\")\n",
        "\n",
        "# Call the function to download names\n",
        "download_names()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35LDrlyB_uiu",
        "outputId": "cd5dad3d-de03-4918-d5ab-169e14106c49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 3850 male names and 4235 female names.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Define the NameDataset class to load and process name datasets\n",
        "class NameDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.names = f.read().splitlines()\n",
        "\n",
        "        # Create char-to-int and int-to-char mappings\n",
        "        self.char_to_int = {char: idx for idx, char in enumerate(set(\"\".join(self.names)))}\n",
        "        self.int_to_char = {idx: char for char, idx in self.char_to_int.items()}\n",
        "        self.vocab_size = len(self.char_to_int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "        int_sequence = [self.char_to_int[char] for char in name]\n",
        "        return torch.tensor(int_sequence)\n",
        "\n",
        "# Define the NameGenerator model (LSTM-based model for name generation)\n",
        "class NameGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
        "        super(NameGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "# Define a function for padding sequences within a batch\n",
        "def pad_collate(batch):\n",
        "    padded_sequences = pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "    targets = padded_sequences[:, 1:]\n",
        "    inputs = padded_sequences[:, :-1]\n",
        "    return inputs, targets\n",
        "\n",
        "# Define a function for training the model\n",
        "\n",
        "def train_model(dataset, model, num_epochs=0, batch_size=32):  # Change num_epochs to 100\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output.contiguous().view(-1, dataset.vocab_size), targets.contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# Train the male model\n",
        "dataset_male = NameDataset('vardai_male.txt')\n",
        "model_male = NameGenerator(dataset_male.vocab_size)\n",
        "train_model(dataset_male, model_male)\n",
        "torch.save(model_male.state_dict(), 'model_male.pth')\n",
        "\n",
        "# Train the female model\n",
        "dataset_female = NameDataset('vardai_female.txt')\n",
        "model_female = NameGenerator(dataset_female.vocab_size)\n",
        "train_model(dataset_female, model_female)\n",
        "torch.save(model_female.state_dict(), 'model_female.pth')\n"
      ],
      "metadata": {
        "id": "onYjGvgFK9g7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG1Y4Yh8K9Wq",
        "outputId": "ba6f82f4-2249-461c-a115-9cb4712cc4d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tmodel_female.pth  model_male.pth  sample_data  vardai_female.txt  vardai_male.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 1. Define the NameDataset class\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, txt_file):\n",
        "        # Read names from the TXT file, one name per line\n",
        "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "            self.names = [line.strip() for line in f] #strip() removes leading/trailing whitespace\n",
        "\n",
        "        # Create a character set from the names and include a space (padding character)\n",
        "        self.chars = sorted(list(set(''.join(self.names)))) + [' ']\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "        # Create dictionaries for character-to-index and index-to-character mapping\n",
        "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: c for i, c in enumerate(self.chars)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx] + ' '  # Adding padding character at the end\n",
        "        encoded_name = [self.char_to_int[char] for char in name]\n",
        "        return torch.tensor(encoded_name)\n"
      ],
      "metadata": {
        "id": "cE2Myd1CFPjC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collate function for padding\n",
        "def pad_collate(batch):\n",
        "    padded_seqs = pad_sequence(batch, batch_first=True, padding_value=0)  # Pad sequences to the same length\n",
        "    input_seq = padded_seqs[:, :-1]  # All but the last character for input\n",
        "    target_seq = padded_seqs[:, 1:]  # All but the first character for target\n",
        "    return input_seq, target_seq\n"
      ],
      "metadata": {
        "id": "gFK-gx7OFS-U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the MinimalTransformer model\n",
        "class MinimalTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):\n",
        "        super(MinimalTransformer, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)  # Embedding layer for character input\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))  # Positional encoding\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)  # Output layer to predict next character\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1)).unsqueeze(0)  # Positional encoding for each input\n",
        "        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :]  # Add positional encoding\n",
        "        x = self.transformer_encoder(x)  # Pass through the transformer encoder\n",
        "        x = self.output_layer(x)  # Output layer to get predictions\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "l8yprJeKFS7a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Training Loop\n",
        "def train_model(model, dataloader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function for classification (predicting the next character)\n",
        "    optimizer = optim.Adam(model.parameters())  # Optimizer (Adam)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            output = model(input_seq)  # Get model predictions\n",
        "            loss = criterion(output.transpose(1, 2), target_seq)  # Compute the loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update the model parameters\n",
        "\n",
        "            total_loss += loss.item()  # Accumulate total loss\n",
        "            batch_count += 1\n",
        "\n",
        "        average_loss = total_loss / batch_count  # Calculate average loss for the epoch\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n"
      ],
      "metadata": {
        "id": "CWdhKyhpFS4J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Sampling function\n",
        "def sample(model, dataset, start_str='a', max_length=10, temperature=1):\n",
        "    assert temperature > 0, \"Temperature must be greater than 0\"\n",
        "    model.eval()  # Switch model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Convert start string to tensor\n",
        "        chars = [dataset.char_to_int[c] for c in start_str]\n",
        "        input_seq = torch.tensor(chars).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        output_name = start_str\n",
        "        for _ in range(max_length - len(start_str)):\n",
        "            output = model(input_seq)\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            logits = output[0, -1] / temperature\n",
        "            probabilities = torch.softmax(logits, dim=0)\n",
        "\n",
        "            # Sample a character from the probability distribution\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.int_to_char[next_char_idx]\n",
        "\n",
        "            if next_char == ' ':  # Stop if end-of-sequence character (space) is reached\n",
        "                break\n",
        "\n",
        "            output_name += next_char\n",
        "            # Update the input sequence for the next iteration\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n",
        "\n",
        "        return output_name\n"
      ],
      "metadata": {
        "id": "AIYXQI-vFPgF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Load the datasets and create dataloaders\n",
        "dataset_male = NameDataset('vardai_male.txt')\n",
        "dataset_female = NameDataset('vardai_female.txt')\n",
        "\n",
        "dataloader_male = DataLoader(dataset_male, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
        "dataloader_female = DataLoader(dataset_female, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
        "\n",
        "# 6. Initialize and train the models for male and female datasets separately\n",
        "\n",
        "# Model for male names\n",
        "model_male = MinimalTransformer(vocab_size=dataset_male.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "train_model(model_male, dataloader_male, epochs=10)\n",
        "\n",
        "# Model for female names\n",
        "model_female = MinimalTransformer(vocab_size=dataset_female.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "train_model(model_female, dataloader_female, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCYgiZ_sFPc9",
        "outputId": "aebce69d-dfa5-46b3-f4ad-14c14553257c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 1.5472252625079195\n",
            "Epoch 2, Average Loss: 1.306915528025509\n",
            "Epoch 3, Average Loss: 1.281148790820571\n",
            "Epoch 4, Average Loss: 1.2563166007522708\n",
            "Epoch 5, Average Loss: 1.2453235506026212\n",
            "Epoch 6, Average Loss: 1.2437746657812891\n",
            "Epoch 7, Average Loss: 1.236407816902665\n",
            "Epoch 8, Average Loss: 1.240038552560097\n",
            "Epoch 9, Average Loss: 1.2262902560312885\n",
            "Epoch 10, Average Loss: 1.2238801299047863\n",
            "Epoch 1, Average Loss: 1.6172374746853249\n",
            "Epoch 2, Average Loss: 1.3963527544996792\n",
            "Epoch 3, Average Loss: 1.3697000051799573\n",
            "Epoch 4, Average Loss: 1.3459201441671615\n",
            "Epoch 5, Average Loss: 1.3384021305500116\n",
            "Epoch 6, Average Loss: 1.3347384786247312\n",
            "Epoch 7, Average Loss: 1.3272630101756047\n",
            "Epoch 8, Average Loss: 1.320498504136738\n",
            "Epoch 9, Average Loss: 1.325978676179298\n",
            "Epoch 10, Average Loss: 1.315123378782344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Generate names with the trained models\n",
        "\n",
        "# Generate male names\n",
        "print('Male names:')\n",
        "for _ in range(10):\n",
        "    generated_name_male = sample(model_male, dataset_male, start_str='A', temperature=0.5)\n",
        "    print(generated_name_male)\n",
        "\n",
        "# Generate female names\n",
        "print('\\nFemale names:')\n",
        "for _ in range(10):\n",
        "    generated_name_female = sample(model_female, dataset_female, start_str='At', temperature=1)\n",
        "    print(generated_name_female)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K227iXzFPZI",
        "outputId": "d71e6fe4-69b9-416f-9bbc-768ccd90d104"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Male names:\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "AAAAAAAAAA\n",
            "\n",
            "Female names:\n",
            "Atelė\n",
            "Atìlì\n",
            "Atìnė́nija\n",
            "Atedmatė\n",
            "Atãsė\n",
            "Atì\n",
            "Atãžė\n",
            "Atene\n",
            "Ateosė\n",
            "Atrẽtė\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "dataset_male = NameDataset('vardai_male.txt')\n",
        "dataset_female = NameDataset('vardai_female.txt')\n",
        "\n",
        "# Initialize dataloaders\n",
        "dataloader_male = DataLoader(dataset_male, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
        "dataloader_female = DataLoader(dataset_female, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
        "\n",
        "# Initialize models\n",
        "model_male = MinimalTransformer(vocab_size=dataset_male.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "model_female = MinimalTransformer(vocab_size=dataset_female.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "\n",
        "# Train models\n",
        "train_model(model_male, dataloader_male, epochs=50)\n",
        "train_model(model_female, dataloader_female, epochs=50)\n",
        "\n",
        "# Save models\n",
        "torch.save(model_male.state_dict(), 'model_male.pth')\n",
        "torch.save(model_female.state_dict(), 'model_female.pth')\n",
        "\n",
        "print(\"Models saved as 'model_male.pth' and 'model_female.pth'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD622dReLGHg",
        "outputId": "373e8fe0-44d3-42c1-feda-ac45b570a78a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 1.5628551628964007\n",
            "Epoch 2, Average Loss: 1.3031216712037395\n",
            "Epoch 3, Average Loss: 1.2637528703232441\n",
            "Epoch 4, Average Loss: 1.2639398909797352\n",
            "Epoch 5, Average Loss: 1.248161835611359\n",
            "Epoch 6, Average Loss: 1.2535044339077532\n",
            "Epoch 7, Average Loss: 1.236381307613751\n",
            "Epoch 8, Average Loss: 1.2319335986760036\n",
            "Epoch 9, Average Loss: 1.2153853685402674\n",
            "Epoch 10, Average Loss: 1.2235587431379586\n",
            "Epoch 11, Average Loss: 1.224590501509422\n",
            "Epoch 12, Average Loss: 1.2308696094623282\n",
            "Epoch 13, Average Loss: 1.2175002925652119\n",
            "Epoch 14, Average Loss: 1.2109722564043093\n",
            "Epoch 15, Average Loss: 1.2168528910510796\n",
            "Epoch 16, Average Loss: 1.2166226151560948\n",
            "Epoch 17, Average Loss: 1.2082349288562113\n",
            "Epoch 18, Average Loss: 1.2043270857866146\n",
            "Epoch 19, Average Loss: 1.204900816945005\n",
            "Epoch 20, Average Loss: 1.2142493463744801\n",
            "Epoch 21, Average Loss: 1.199482084798419\n",
            "Epoch 22, Average Loss: 1.1995751217377086\n",
            "Epoch 23, Average Loss: 1.2083317706407595\n",
            "Epoch 24, Average Loss: 1.206308583090128\n",
            "Epoch 25, Average Loss: 1.2037875085822807\n",
            "Epoch 26, Average Loss: 1.1981617872380028\n",
            "Epoch 27, Average Loss: 1.2065274188341188\n",
            "Epoch 28, Average Loss: 1.1983220197937705\n",
            "Epoch 29, Average Loss: 1.1995506611737339\n",
            "Epoch 30, Average Loss: 1.2097455228655791\n",
            "Epoch 31, Average Loss: 1.1988494297689642\n",
            "Epoch 32, Average Loss: 1.1974151750241429\n",
            "Epoch 33, Average Loss: 1.194728275961127\n",
            "Epoch 34, Average Loss: 1.1991146463007967\n",
            "Epoch 35, Average Loss: 1.1866385133798458\n",
            "Epoch 36, Average Loss: 1.1947120748275568\n",
            "Epoch 37, Average Loss: 1.2029887147186216\n",
            "Epoch 38, Average Loss: 1.1935909819011845\n",
            "Epoch 39, Average Loss: 1.1912055281568166\n",
            "Epoch 40, Average Loss: 1.196102581240914\n",
            "Epoch 41, Average Loss: 1.1913580786098132\n",
            "Epoch 42, Average Loss: 1.1930105873375885\n",
            "Epoch 43, Average Loss: 1.1958256299830665\n",
            "Epoch 44, Average Loss: 1.192796363318262\n",
            "Epoch 45, Average Loss: 1.19209455606366\n",
            "Epoch 46, Average Loss: 1.1861621565070033\n",
            "Epoch 47, Average Loss: 1.1951938261670514\n",
            "Epoch 48, Average Loss: 1.1962659388534294\n",
            "Epoch 49, Average Loss: 1.1899191131276532\n",
            "Epoch 50, Average Loss: 1.1946614901881574\n",
            "Epoch 1, Average Loss: 1.6302332062470286\n",
            "Epoch 2, Average Loss: 1.3883012694523746\n",
            "Epoch 3, Average Loss: 1.3638629850588346\n",
            "Epoch 4, Average Loss: 1.348382632535203\n",
            "Epoch 5, Average Loss: 1.3436494061821385\n",
            "Epoch 6, Average Loss: 1.3358531509126936\n",
            "Epoch 7, Average Loss: 1.323817321232387\n",
            "Epoch 8, Average Loss: 1.318077904837472\n",
            "Epoch 9, Average Loss: 1.3211666255965269\n",
            "Epoch 10, Average Loss: 1.3160061074378795\n",
            "Epoch 11, Average Loss: 1.3052837543917777\n",
            "Epoch 12, Average Loss: 1.3176529815322475\n",
            "Epoch 13, Average Loss: 1.304532267545399\n",
            "Epoch 14, Average Loss: 1.3080049197476609\n",
            "Epoch 15, Average Loss: 1.3050052922471125\n",
            "Epoch 16, Average Loss: 1.3031900233792184\n",
            "Epoch 17, Average Loss: 1.3060489005612252\n",
            "Epoch 18, Average Loss: 1.3004421811354787\n",
            "Epoch 19, Average Loss: 1.300399821503718\n",
            "Epoch 20, Average Loss: 1.299003701460989\n",
            "Epoch 21, Average Loss: 1.2924701468388837\n",
            "Epoch 22, Average Loss: 1.3009651899337769\n",
            "Epoch 23, Average Loss: 1.292729918221782\n",
            "Epoch 24, Average Loss: 1.3043176231527687\n",
            "Epoch 25, Average Loss: 1.2984277561194915\n",
            "Epoch 26, Average Loss: 1.2971534684188384\n",
            "Epoch 27, Average Loss: 1.2969027605271877\n",
            "Epoch 28, Average Loss: 1.291578201423014\n",
            "Epoch 29, Average Loss: 1.288174727805575\n",
            "Epoch 30, Average Loss: 1.2999240545401896\n",
            "Epoch 31, Average Loss: 1.301499389168015\n",
            "Epoch 32, Average Loss: 1.2927012291169704\n",
            "Epoch 33, Average Loss: 1.2959053883875222\n",
            "Epoch 34, Average Loss: 1.2844138486044747\n",
            "Epoch 35, Average Loss: 1.2864145040512085\n",
            "Epoch 36, Average Loss: 1.2783364998666864\n",
            "Epoch 37, Average Loss: 1.285101886530568\n",
            "Epoch 38, Average Loss: 1.2896381419404108\n",
            "Epoch 39, Average Loss: 1.2800448657874774\n",
            "Epoch 40, Average Loss: 1.2828595602422728\n",
            "Epoch 41, Average Loss: 1.293841584732658\n",
            "Epoch 42, Average Loss: 1.2906850052059144\n",
            "Epoch 43, Average Loss: 1.2835629725814761\n",
            "Epoch 44, Average Loss: 1.2888656289953935\n",
            "Epoch 45, Average Loss: 1.2823123205873304\n",
            "Epoch 46, Average Loss: 1.28249936892574\n",
            "Epoch 47, Average Loss: 1.2917076761561228\n",
            "Epoch 48, Average Loss: 1.2918322597231184\n",
            "Epoch 49, Average Loss: 1.2793473973310083\n",
            "Epoch 50, Average Loss: 1.281782607386883\n",
            "Models saved as 'model_male.pth' and 'model_female.pth'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Male names:')\n",
        "for _ in range(10):\n",
        "    generated_name_male = sample(model_male, dataset_male, start_str='A', temperature= 1)\n",
        "    print(generated_name_male)\n",
        "\n",
        "# Generate female names\n",
        "print('\\nFemale names:')\n",
        "for _ in range(10):\n",
        "    generated_name_female = sample(model_female, dataset_female, start_str='Be', temperature=1)\n",
        "    print(generated_name_female)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4JwOEPURrqk",
        "outputId": "7c0b31dc-9a4a-4891-d99b-36b86fbdf093"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Male names:\n",
            "Arovintãn\n",
            "Ãdvijus\n",
            "Aurvijus\n",
            "Ãdelaris\n",
            "Aimántas\n",
            "Ailmastas\n",
            "Aìnius\n",
            "Ãrfimis\n",
            "Aìntas\n",
            "Ãgaras\n",
            "\n",
            "Female names:\n",
            "Belve\n",
            "Belingė\n",
            "Berija\n",
            "Bevicija\n",
            "Bečidzijà\n",
            "Belė\n",
            "Berija\n",
            "Beina\n",
            "Beviū́ija\n",
            "Beninė\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import streamlit as st\n",
        "\n",
        "# 1. Define the MinimalTransformer model\n",
        "class MinimalTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):\n",
        "        super(MinimalTransformer, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)  # Embedding layer for character input\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))  # Positional encoding\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)  # Output layer to predict next character\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1)).unsqueeze(0)  # Positional encoding for each input\n",
        "        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :]  # Add positional encoding\n",
        "        x = self.transformer_encoder(x)  # Pass through the transformer encoder\n",
        "        x = self.output_layer(x)  # Output layer to get predictions\n",
        "        return x\n",
        "\n",
        "\n",
        "# 2. Define the NameDataset class\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, txt_file):\n",
        "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "            self.names = [line.strip() for line in f]\n",
        "        self.chars = sorted(list(set(''.join(self.names)))) + [' ']\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: c for i, c in enumerate(self.chars)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx] + ' '\n",
        "        encoded_name = [self.char_to_int[char] for char in name]\n",
        "        return torch.tensor(encoded_name)\n",
        "\n",
        "\n",
        "# Function to generate a name from the trained model\n",
        "def generate_name(model, dataset, start_chars, max_length=30, temperature=1.0):\n",
        "    model.eval()\n",
        "    start_chars = start_chars.capitalize()  # Ensure only the first character is uppercase\n",
        "    input_tensor = torch.tensor([dataset.char_to_int[char] for char in start_chars], dtype=torch.long).unsqueeze(0)\n",
        "    generated_name = start_chars\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length - len(start_chars)):\n",
        "            output = model(input_tensor)\n",
        "            logits = output[0, -1, :] / temperature  # Adjust temperature to control randomness\n",
        "            probabilities = F.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "            # Sample from the probabilities to choose the next character\n",
        "            predicted_char_idx = torch.multinomial(torch.tensor(probabilities), num_samples=1).item()\n",
        "            predicted_char = dataset.int_to_char[predicted_char_idx]\n",
        "\n",
        "            if predicted_char == ' ':\n",
        "                break\n",
        "            generated_name += predicted_char\n",
        "            input_tensor = torch.cat([input_tensor, torch.tensor([[predicted_char_idx]], dtype=torch.long)], dim=1)\n",
        "\n",
        "    return generated_name\n",
        "\n",
        "\n",
        "# Streamlit interface\n",
        "st.title(\"Lietuviškų vardų generatorius\")\n",
        "\n",
        "# User selects name type\n",
        "name_type = st.selectbox(\"Pasirinkite vardo tipą\", [\"Vyriškas\", \"Moteriškas\"])\n",
        "\n",
        "# Load the correct dataset and model\n",
        "if not os.path.exists('model_male.pth') or not os.path.exists('model_female.pth'):\n",
        "    st.error(\"Model files are missing. Please ensure 'model_male.pth' and 'model_female.pth' are in the correct directory.\")\n",
        "else:\n",
        "    if name_type == \"Vyriškas\":\n",
        "        dataset = NameDataset('vardai_male.txt')\n",
        "        model = MinimalTransformer(vocab_size=dataset.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "        model.load_state_dict(torch.load('model_male.pth'), strict=False)\n",
        "    else:\n",
        "        dataset = NameDataset('vardai_female.txt')\n",
        "        model = MinimalTransformer(vocab_size=dataset.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "        model.load_state_dict(torch.load('model_female.pth'), strict=False)\n",
        "\n",
        "    # Input field for the starting letter(s)\n",
        "    start_chars = st.text_input(\"Įveskite pradines raides\", \"\")\n",
        "\n",
        "    # Slider for temperature\n",
        "\n",
        "    temperature = st.slider(\"Pasirinkite atsitiktinumo lygį (temperatūra)\", 0.5, 2.0, 1.0)\n",
        "\n",
        "    # Button to generate a name\n",
        "    if st.button(\"Generuoti vardą\"):\n",
        "        if start_chars:\n",
        "            # Generate and display the name\n",
        "            name = generate_name(model, dataset, start_chars, temperature=temperature)\n",
        "            st.write(f\"Sugeneruotas Vardas: {name}\")\n",
        "        else:\n",
        "            st.warning(\"Prašome įvesti pradines raides!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTf8L0k2Vqt4",
        "outputId": "7fc3cc30-174a-482e-a57c-91c493321695"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Configure ngrok with your authentication token\n",
        "ngrok.set_auth_token(\"2pA2nwpPDy9sd9LDIWbHgGVGePk_7r2jWaiKrYhnRDMpQd7r1\")\n",
        "\n",
        "# Configure ngrok to use the default free server region\n",
        "conf.get_default().region = \"us\"\n",
        "\n",
        "# Set up the Streamlit tunnel on port 8501 (the default for Streamlit)\n",
        "tunnel = ngrok.connect(8501)\n",
        "\n",
        "# Get the public URL of the tunnel\n",
        "public_url = tunnel.public_url\n",
        "print(f\"Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rbZNlQfWWBX",
        "outputId": "770df242-8444-4424-fae4-a965b1dd9474"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: https://4930-34-125-74-18.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMXPioosWcxv",
        "outputId": "3e49567b-f21c-4e19-8b1e-78d4f3125f5d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.74.18:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/content/app.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model_male.pth'), strict=False)\n",
            "2024-12-03 13:34:26.699 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-12-03T13:34:40+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-117c69f3-99b3-497a-be99-9d4c47c8bab5 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-12-03T13:34:40+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-117c69f3-99b3-497a-be99-9d4c47c8bab5 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WF5loh6AieoJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}